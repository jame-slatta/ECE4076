{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera models and Homographies\n",
    "\n",
    "In this weeks lectures, we started to look at camera models. Lets explore these a bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "from math import cos, sin\n",
    "\n",
    "from jupyterquiz import display_quiz\n",
    "import json\n",
    "with open(\"./Questions_week_4.json\", \"r\") as file:\n",
    "    questions = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets imagine I have a camera with the following intrinsic matrix:\n",
    "\n",
    "$\\mathbf{K} = \\begin{bmatrix} 1200 & 0 & 340\\\\ 0 & 1200 & 240\\\\ 0 & 0 & 1 \\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz(questions[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection\n",
    "Lets imagine a set of points on a curve in 3D space. Assuming a camera is directly in front of the curve, how will this curve appear in our image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10, 7))\n",
    "ax = plt.axes(projection =\"3d\")\n",
    "\n",
    "x = np.linspace(-0.1,0.1,100)\n",
    "y = 0.1*np.sin(50*x)\n",
    "z = np.ones(100)\n",
    "\n",
    "ax.scatter3D(x,y,z)\n",
    "ax.plot([0,0.05],[0,0],[0,0],'r')\n",
    "ax.plot([0,0],[0,0.05],[0,0],'g')\n",
    "ax.plot([0,0],[0,0],[0,0.05],'b')\n",
    "plt.xlim(-0.5,0.5)\n",
    "plt.ylim(-0.5,0.5)\n",
    "ax.set_zlim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.array([[1200,0,340],[0,1200,240],[0,0,1]])\n",
    "\n",
    "# Create a coordinate vector for all points\n",
    "X = np.vstack([x,y,z])\n",
    "\n",
    "print(\"We now have 100 different coordinate vectors:\",X.shape)\n",
    "\n",
    "# Project into image plane\n",
    "im_coords_scaled = K@X\n",
    "im_coords = im_coords_scaled/im_coords_scaled[2,:]\n",
    "\n",
    "plt.plot(im_coords[0,:],im_coords[1,:],'o')\n",
    "plt.xlim(0,640)\n",
    "plt.ylim(0,480)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if our camera rotates or translates slightly? Lets consider the full case, where we first rotate and translate before projection.\n",
    "\n",
    "$\\mathbf{u} = \\mathbf{K}\\begin{bmatrix}\\mathbf{R} & \\mathbf{t}\\end{bmatrix} \\mathbf{X} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.array([[1200,0,340],[0,1200,240],[0,0,1]])\n",
    "\n",
    "\n",
    "def get_transformation_matrix(roll,pitch,yaw,tx,ty,tz):\n",
    "    return np.array(\n",
    "            [[cos(yaw) * cos(pitch), -sin(yaw) * cos(roll) + cos(yaw) * sin(pitch) * sin(roll), sin(yaw) * sin(roll) + cos(yaw) * sin(pitch) * cos(roll), tx],\n",
    "             [sin(yaw) * cos(pitch), cos(yaw) * cos(roll) + sin(yaw) * sin(pitch)\n",
    "              * sin(roll), -cos(yaw) * sin(roll) + sin(yaw) * sin(pitch) * cos(roll), ty],\n",
    "             [-sin(pitch), cos(pitch) * sin(roll), cos(pitch) * cos(yaw), tz]\n",
    "             ])\n",
    "\n",
    "# Rotate camera 45 degrees and pull it back 0.5 m\n",
    "T = get_transformation_matrix(0,0,np.pi/4,0,0,0.5)\n",
    "\n",
    "print(\"Transformation matrix: \\n\",T)\n",
    "# Create a homogenous coordinate vector for all points\n",
    "X = np.vstack([x,y,z,np.ones(100)])\n",
    "\n",
    "print(\"We now have 100 different homogenous coordinate vectors:\",X.shape)\n",
    "\n",
    "\n",
    "# Project into image plane\n",
    "im_coords_scaled = K@T@X\n",
    "im_coords = im_coords_scaled/im_coords_scaled[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(im_coords[0,:],im_coords[1,:],'o')\n",
    "plt.xlim(0,640)\n",
    "plt.ylim(0,480)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2\n",
    "\n",
    "Modify the code above to try out different rotation and translations. \n",
    "\n",
    "- How does this affect the camera projection.\n",
    "- What is translating and rotating, the camera or the points?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz(questions[4:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ok, why is this important? Well, if we can detect points in an image, and know something about the object dimensions or structure, we can say something about it's position and orientation. This is valuable for a range of applications, eg. a robot grasping an object. We'll also use this knowledge later on, to build 3D models of objects from multiple views.\n",
    "\n",
    "As another example, assuming you know the camera intrinsics, and that a golf ball is 42 mm in diameter, and you can find find the diameter of an ball in an image, you can estimate how far away the ball is from a camera. \n",
    "\n",
    "### Try this at home: \n",
    "So how do we find our camera intrinsics?\n",
    "- [Calibrate](https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html) your webcam to find the matrix $\\mathbf{K}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homographies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we jump into multi-view geometry and 3D geometry, lets think about planar geometry and operations. We already encountered these operations last week, when we warped images using shear, affine and rotational transformations. This week we'll look at the reverse approach, lets look at how to recover a homography, from corresponding point matches. We'll use an ORB descriptor built into opencv, which improves on the brief descriptor we used last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_1 = cv2.cvtColor(cv2.resize(cv2.imread('../test_images/Drone_1.jpg'),(400,400)), cv2.COLOR_BGR2GRAY)\n",
    "im_2 = cv2.cvtColor(cv2.resize(cv2.imread('../test_images/Drone_2.jpg'),(400,400)), cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Find the keypoints and descriptors\n",
    "kp1, des1 = orb.detectAndCompute(im_1,None)\n",
    "kp2, des2 = orb.detectAndCompute(im_2,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match keypoints\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1,des2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort matches\n",
    "matches = sorted(matches, key = lambda x:x.distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw first 20 matches.\n",
    "plt.figure(figsize=(15,5))\n",
    "img3 = cv2.drawMatches(im_1,kp1,im_2,kp2,matches[:],None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(img3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find keypoint correspondences\n",
    "X1 = np.vstack([kp1[match.queryIdx].pt for match in matches])\n",
    "X2 = np.vstack([kp2[match.trainIdx].pt for match in matches])\n",
    "\n",
    "# Estimate homograpahy using opencv - \n",
    "#Hcv, mask = cv2.findHomography(X2, X1, cv2.RANSAC, 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we have two N x 2 of corresponding keypoints. We're now ready to compute the homography between these.\n",
    "In opencv this is easy \n",
    "\n",
    "Hcv, mask = cv2.findHomography(X2, X1, cv2.RANSAC, 5.0) \n",
    "\n",
    "but we'll write our own code to understand this better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix from 1 keypoint correspondence\n",
    "def point_correspondance_matrix(x,y,u,v):\n",
    "    \n",
    "    row_1 = np.array([-x, -y, -1, 0, 0, 0, x*u, y*u, u])\n",
    "    row_2 = np.array([0, 0, 0, -x, -y, -1, x*v, y*v, v])\n",
    "\n",
    "    return (np.vstack([row_1,row_2]))\n",
    "    \n",
    "# Create a full matrix from multiple keypoint correspondences - technically this works with more than 4 points too\n",
    "def four_point_correspondance_matrix(kp1,kp2):\n",
    "    \n",
    "    rows = []\n",
    "    for i in range(kp1.shape[0]):\n",
    "        rows.append(point_correspondance_matrix(kp1[i,0],kp1[i,1],kp2[i,0],kp2[i,1]))\n",
    "    return np.vstack(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the homography using singular value decomposition\n",
    "def find_homography(kp1,kp2):\n",
    "    \n",
    "    M = four_point_correspondance_matrix(kp1,kp2)\n",
    "    \n",
    "    U,S,Vt  = np.linalg.svd(M)\n",
    "    H = Vt[-1,:].reshape(3,3) #Null space corresponds to smallest singular value\n",
    "    return H/H[2,2] # Typically normalise by the last element of the homography "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a homography using ransac\n",
    "def ransac_homography(kp1,kp2,N=1000,max_error=5.0):\n",
    "    \n",
    "    best_consensus_size = 0\n",
    "    best_H = np.eye(3)\n",
    "    \n",
    "    for j in range(N):\n",
    "        \n",
    "        # Pick 4 points at random\n",
    "        bins = np.random.choice(kp1.shape[0],4,replace=False)\n",
    "        \n",
    "        # Calculate Homography\n",
    "        H = find_homography(kp1[bins,:],kp2[bins,:])\n",
    "        \n",
    "        # Project points using Homography\n",
    "        X1_h = np.hstack((kp1,np.ones((kp1.shape[0],1)))).T # homogenous coordinates\n",
    "        \n",
    "        projected = H@X1_h\n",
    "        # Normalise (last column must be one for homogenous coordinates) \n",
    "        X1_2 = projected[0:2,:]/projected[2,:] \n",
    "        \n",
    "        # Calculate reprojection error\n",
    "        reprojection_errors = np.sqrt(np.sum(np.power(X2-X1_2.T,2),axis=1))\n",
    "        \n",
    "        # Count consensus set size\n",
    "        consensus_size = np.sum(reprojection_errors<max_error)\n",
    "        \n",
    "        # Save current best homography\n",
    "        if consensus_size > best_consensus_size:\n",
    "            best_consensus_size = consensus_size\n",
    "            best_H = H\n",
    "            \n",
    "    return best_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = ransac_homography(X1,X2,N=10000,max_error=5)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets apply the homography to our image - in theory it should generate the second picture \n",
    "im_warp = cv2.warpPerspective(im_1,H,(im_1.shape[0],im_1.shape[1]))\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(im_1)\n",
    "plt.title('Image 1')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(im_warp)\n",
    "plt.title('Image 1 warped by Homography')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(im_2)\n",
    "plt.title('Image 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, that warped our first image to make it look like our second image! Think the warp-perspective function is magic? Don't worry, in one of the labs you'll write the code to implement this. :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3 \n",
    "\n",
    "Increase the maximum error used to find the consensus set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz(questions[8:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
